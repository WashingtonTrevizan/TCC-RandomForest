#!/usr/bin/env python
"""
ğŸ”´ CENÃRIO 2: TESTAR MODELO COM DATASET DIFERENTE
=================================================

Script para executar APENAS o teste do modelo:
1. Gerar dataset de teste (diferente do treinamento)
2. Carregar modelo previamente treinado
3. Executar teste e avaliar performance
4. Gerar relatÃ³rio de resultados

ExecuÃ§Ã£o: python executar_cenario_2_testar.py

âš ï¸ PREREQUISITO: CenÃ¡rio 1 deve ter sido executado primeiro
"""

import os
import subprocess
import sys
from pathlib import Path

def verificar_prerequisites():
    """Verifica se o modelo foi treinado"""
    arquivos_necessarios = [
        "data/models/ddos_model_realista.pkl",
        "data/models/scaler_realista.joblib", 
        "data/models/label_mapping_realista.json"
    ]
    
    for arquivo in arquivos_necessarios:
        if not os.path.exists(arquivo):
            return False, arquivo
    
    return True, None

def executar_cenario_testar():
    """Executa cenÃ¡rio completo de teste"""
    
    print("ğŸ”´ CENÃRIO 2: TESTE DE MODELO")
    print("=" * 60)
    print("ğŸ¯ Objetivo: Dataset Novo â†’ Carregar Modelo â†’ Testar â†’ RelatÃ³rio")
    print()
    
    # Verificar prerequisites
    print("ğŸ” Verificando prerequisites...")
    tem_modelo, arquivo_faltando = verificar_prerequisites()
    
    if not tem_modelo:
        print(f"âŒ Modelo nÃ£o encontrado: {arquivo_faltando}")
        print("âš ï¸ Execute primeiro o cenÃ¡rio 1:")
        print("   python executar_cenario_1_treinar.py")
        return False
    
    print("âœ… Modelo treinado encontrado!")
    print()
    
    # Verificar se estamos no diretÃ³rio correto
    if not os.path.exists("experiments"):
        print("âŒ Erro: Execute este script da raiz do projeto")
        return False
    
    try:
        print("ğŸ“Š PASSO 1/2: Gerando dataset de teste (diferente do treino)...")
        print("-" * 50)
        result = subprocess.run([
            sys.executable, "experiments/gerar_trafego_teste.py"
        ], capture_output=True, text=True, cwd=".")
        
        if result.returncode != 0:
            print(f"âŒ Erro na geraÃ§Ã£o de dados de teste: {result.stderr}")
            return False
        
        print("âœ… Dataset de teste gerado com sucesso!")
        print(f"ğŸ“ Salvo em: data/raw/test_traffic_realistic.csv")
        print()
        
        print("ğŸ§ª PASSO 2/2: Testando modelo com dataset novo...")
        print("-" * 50)
        result = subprocess.run([
            sys.executable, "tools/teste_definitivo.py"
        ], capture_output=True, text=True, cwd=".")
        
        if result.returncode != 0:
            print(f"âŒ Erro no teste: {result.stderr}")
            return False
            
        print("âœ… Teste executado com sucesso!")
        print()
        
        print("ğŸ‰ CENÃRIO 2 COMPLETO!")
        print("=" * 60)
        print("ğŸ“¦ ARQUIVOS GERADOS:")
        arquivos_teste = [
            "data/raw/test_traffic_realistic.csv - Dataset de teste",
            "data/processed/test_features_realistic.csv - Features de teste",
            "data/processed/teste_modelo_trafego_realista.csv - Resultados detalhados"
        ]
        
        for arquivo in arquivos_teste:
            caminho = arquivo.split(" - ")[0]
            if os.path.exists(caminho):
                print(f"  âœ… {arquivo}")
            else:
                print(f"  âŒ {arquivo}")
        
        print()
        print("ğŸ“Š ANÃLISE DOS RESULTADOS:")
        
        # Tentar mostrar resumo se o arquivo existe
        resultado_path = "data/processed/teste_modelo_trafego_realista.csv"
        if os.path.exists(resultado_path):
            try:
                import pandas as pd
                df = pd.read_csv(resultado_path)
                accuracy = df['Correto'].mean()
                total_samples = len(df)
                
                print(f"   ğŸ¯ Amostras testadas: {total_samples:,}")
                print(f"   ğŸ“ˆ AcurÃ¡cia geral: {accuracy:.3f} ({accuracy*100:.1f}%)")
                
                # DistribuiÃ§Ã£o por classe
                print(f"   ğŸ“Š DistribuiÃ§Ã£o dos dados de teste:")
                label_counts = df['Label'].value_counts()
                for label, count in label_counts.items():
                    correct = df[df['Label'] == label]['Correto'].sum()
                    class_acc = (correct / count) * 100
                    print(f"      {label}: {count:,} amostras ({class_acc:.1f}% acertos)")
                    
            except Exception as e:
                print(f"   âš ï¸ NÃ£o foi possÃ­vel analisar resultados: {e}")
        
        print()
        print("ğŸ“‹ RELATÃ“RIOS DISPONÃVEIS:")
        print("   ğŸ“„ Resultados detalhados: data/processed/teste_modelo_trafego_realista.csv")
        print("   ğŸ“„ RelatÃ³rio completo: Execute tools/gerar_relatorio_final.py")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erro inesperado: {e}")
        return False

if __name__ == "__main__":
    print("ğŸ”´ INICIANDO CENÃRIO 2: TESTE")
    print("=" * 60)
    
    success = executar_cenario_testar()
    
    if success:
        print("\nğŸ‰ CENÃRIO 2 EXECUTADO COM SUCESSO!")
        print("ğŸ“Š Teste completo do modelo finalizado")
        print("ğŸ“‹ Verifique os relatÃ³rios gerados")
    else:
        print("\nâŒ FALHA NO CENÃRIO 2!")
        print("ğŸ”§ Verifique os erros e prerequisites")
    
    print("=" * 60)
